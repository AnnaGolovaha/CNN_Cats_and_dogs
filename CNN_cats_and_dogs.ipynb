{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision as tv\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2class(torch.utils.data.Dataset): #2 клааса, наследуем \n",
    "    def  __init__(self, path_cats:str, path_dogs:str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_cats = path_cats\n",
    "        self.path_dogs = path_dogs\n",
    "\n",
    "        self.cats_list = sorted(os.listdir(path_cats))\n",
    "        self.dogs_list = sorted(os.listdir(path_dogs))\n",
    "\n",
    "    def __len__(self): #функция для получения длины датасета\n",
    "        return len(self.cats_list) + len(self.dogs_list)\n",
    "\n",
    "    def __getitem__(self,idx): #для получения индексов, чтобы использовать как массив\n",
    "        if idx < len(self.cats_list):\n",
    "            class_id = 0\n",
    "            img_path = os.path.join(self.path_cats, self.cats_list[idx])\n",
    "        else:\n",
    "            class_id = 1\n",
    "            idx-=len(self.cats_list) #чтобы обращаться ко второй папке с 0-го элемента\n",
    "            #обе папки стоят подряд в датасете\n",
    "\n",
    "            img_path = os.path.join(self.path_dogs, self.dogs_list[idx]) #склеиваем два пути в один\n",
    "                                                                    #сначала название папки общей, затем уже конкретно коты или собаки\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = img.astype(np.float32) #значения пикселей храним в вещественном формате\n",
    "        img = img/255.0 #нормализуем от 0 до 1\n",
    "\n",
    "        img = cv2.resize(img, (64,64), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        img = img.transpose((2,0,1))\n",
    "\n",
    "        t_img = torch.from_numpy(img)\n",
    "        t_class_id = torch.tensor(class_id)\n",
    "\n",
    "\n",
    "        return {'img': t_img, 'label' : t_class_id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats_path = '.\\dataset_cats_and_dogs\\\\train\\cats'\n",
    "train_dogs_path = '.\\dataset_cats_and_dogs\\\\train\\dogs'\n",
    "\n",
    "train_ds_cats_dogs = Dataset2class(train_cats_path, train_dogs_path)\n",
    "\n",
    "test_cats_path = '.\\dataset_cats_and_dogs\\\\test\\cats'\n",
    "test_dogs_path = '.\\dataset_cats_and_dogs\\\\test\\dogs'\n",
    "\n",
    "test_ds_cats_dogs = Dataset2class(test_cats_path, test_dogs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_ds_cats_dogs[2][0]) #это кортеж, поэтому сначала выбираем номер кортежа, затем элемент в нём\n",
    "# plt.imshow(train_ds_cats_dogs[2]['img']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0720, 0.0782, 0.0784,  ..., 0.0645, 0.0627, 0.0627],\n",
       "         [0.0720, 0.0782, 0.0784,  ..., 0.0645, 0.0627, 0.0627],\n",
       "         [0.0720, 0.0782, 0.0784,  ..., 0.0645, 0.0627, 0.0627],\n",
       "         ...,\n",
       "         [0.1739, 0.2013, 0.1602,  ..., 0.0510, 0.0510, 0.0476],\n",
       "         [0.1804, 0.2138, 0.1499,  ..., 0.0469, 0.0469, 0.0435],\n",
       "         [0.1867, 0.2064, 0.1010,  ..., 0.0431, 0.0431, 0.0398]],\n",
       "\n",
       "        [[0.0759, 0.0821, 0.0824,  ..., 0.0684, 0.0667, 0.0667],\n",
       "         [0.0759, 0.0821, 0.0824,  ..., 0.0684, 0.0667, 0.0667],\n",
       "         [0.0759, 0.0821, 0.0824,  ..., 0.0684, 0.0667, 0.0667],\n",
       "         ...,\n",
       "         [0.1902, 0.2085, 0.1607,  ..., 0.0527, 0.0510, 0.0476],\n",
       "         [0.1978, 0.2210, 0.1504,  ..., 0.0486, 0.0469, 0.0435],\n",
       "         [0.2041, 0.2137, 0.1015,  ..., 0.0449, 0.0431, 0.0398]],\n",
       "\n",
       "        [[0.0955, 0.1017, 0.1020,  ..., 0.0880, 0.0863, 0.0863],\n",
       "         [0.0955, 0.1017, 0.1020,  ..., 0.0880, 0.0863, 0.0863],\n",
       "         [0.0955, 0.1017, 0.1020,  ..., 0.0880, 0.0863, 0.0863],\n",
       "         ...,\n",
       "         [0.2042, 0.2185, 0.1640,  ..., 0.0605, 0.0588, 0.0555],\n",
       "         [0.2112, 0.2299, 0.1515,  ..., 0.0564, 0.0547, 0.0514],\n",
       "         [0.2175, 0.2226, 0.1026,  ..., 0.0527, 0.0510, 0.0476]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_cats_dogs[2]['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds_cats_dogs, shuffle = True, \n",
    "    batch_size = batch_size, num_workers = 0, drop_last = True\n",
    ") #drop_last = True - выбрасываем последний элемент\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_ds_cats_dogs, shuffle = True, \n",
    "    batch_size = batch_size, num_workers = 0, drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d (2,2)\n",
    "        self.conv0 = nn.Conv2d(3,32,3, stride=1, padding = 0)\n",
    "        self.conv1 = nn.Conv2d(32,32,3, stride=1, padding = 0)\n",
    "        self.conv2 = nn.Conv2d(32,64,3, stride=1, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(64,64,3, stride=1, padding = 0)\n",
    "        self.conv4 = nn.Conv2d(64,64,3, stride=1, padding = 0)\n",
    "\n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1=nn.Linear(64,10)\n",
    "        self.linear2=nn.Linear(10,2) #только 2 класса на выходе\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv0(x)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act(out)\n",
    "       \n",
    "        out = self.adaptivepool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (act): LeakyReLU(negative_slope=0.2)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (adaptivepool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear1): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (linear2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_loader:\n",
    "    img = sample['img']\n",
    "    label = sample['label']\n",
    "    net(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr =0.0001, betas = (0.9, 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, label):\n",
    "  answer = F.softmax(pred.detach()).numpy().argmax(1) == label.numpy().argmax(1)\n",
    "\n",
    "  # detach() удаляет граф вычислений (как бы историю) с тензора. Отвязывает.\n",
    "# Back propagation не будет идти дальше этого тензора.\n",
    "\n",
    "  return answer.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameers(model):\n",
    "  return sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103168"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameers(net) #число параметров сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]C:\\Users\\ag\\AppData\\Local\\Temp\\ipykernel_9620\\259133754.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  answer = F.softmax(pred.detach()).numpy().argmax(1) == label.numpy().argmax(1)\n",
      "100%|██████████| 34/34 [00:11<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6986114172374501\n",
      "0.4963235294117647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6963829433216768\n",
      "0.5018382352941176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6964621333514943\n",
      "0.4944852941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6939063019612256\n",
      "0.48713235294117646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6921646472285775\n",
      "0.5330882352941176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6962359074284049\n",
      "0.48161764705882354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7050415680688971\n",
      "0.5055147058823529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9105549314442802\n",
      "0.4889705882352941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.16813958918347\n",
      "0.5238970588235294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:10<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.484831049161799\n",
      "0.5202205882352942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#   loss_val = 0\n",
    "#   acc_val = 0\n",
    "#   # тут будут пачки по 16 сэмплов\n",
    "#   for img, label in (pbar := tqdm(dataloader)): #для каждого изображения считается loss и градиент\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  loss_val = 0 \n",
    "  acc_val = 0\n",
    "  for sample in (pbar := tqdm(train_loader)):\n",
    "    img, label = sample['img'], sample['label']\n",
    "    optimizer.zero_grad()  #обновляем градиенты\n",
    "\n",
    "\n",
    "    label = (torch.Tensor(F.one_hot(label, 2))).float() \n",
    "\n",
    "\n",
    "    pred = net(img)\n",
    "\n",
    "    loss = loss_fn(pred, label)\n",
    "\n",
    "    loss.backward()\n",
    "    loss_item = loss.item()\n",
    "    loss_val += loss_item\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    acc_current = accuracy(pred, label)\n",
    "    acc_val += acc_current #мы же выводим среднее, поэтому суммируем\n",
    "\n",
    "\n",
    "  \n",
    "  pbar.set_description(f'loss: {loss_item:.5f}\\taccuracy: {acc_current:.3f}')\n",
    "\n",
    "  print(loss_val/len(train_loader))\n",
    "  print(acc_val/len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]C:\\Users\\ag\\AppData\\Local\\Temp\\ipykernel_9620\\259133754.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  answer = F.softmax(pred.detach()).numpy().argmax(1) == label.numpy().argmax(1)\n",
      "loss: 2.25524\taccuracy: 0.500: 100%|██████████| 9/9 [00:02<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.782789521747165\n",
      "0.4930555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.60906\taccuracy: 0.333: 100%|██████████| 9/9 [00:02<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8203954564200506\n",
      "0.4884259259259259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.04652\taccuracy: 0.417: 100%|██████████| 9/9 [00:02<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8047695557276406\n",
      "0.49074074074074076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.99020\taccuracy: 0.250: 100%|██████████| 9/9 [00:02<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.830982658598158\n",
      "0.4861111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.56064\taccuracy: 0.667: 100%|██████████| 9/9 [00:02<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.763495127360026\n",
      "0.49768518518518523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.03993\taccuracy: 0.500: 100%|██████████| 9/9 [00:02<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.776808738708496\n",
      "0.4930555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.90178\taccuracy: 0.333: 100%|██████████| 9/9 [00:02<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8007489177915783\n",
      "0.4884259259259259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.92250\taccuracy: 0.500: 100%|██████████| 9/9 [00:02<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7735466162363687\n",
      "0.4930555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.28586\taccuracy: 0.417: 100%|██████████| 9/9 [00:02<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8114178578058877\n",
      "0.49074074074074076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.92759\taccuracy: 0.417: 100%|██████████| 9/9 [00:02<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8014659219317966\n",
      "0.49074074074074076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "  loss_val = 0 \n",
    "  acc_val = 0\n",
    "  for sample in (pbar := tqdm(test_loader)):\n",
    "    with torch.no_grad():\n",
    "        img, label = sample['img'], sample['label']\n",
    "\n",
    "\n",
    "        label = (torch.Tensor(F.one_hot(label, 2))).float() \n",
    "\n",
    "\n",
    "        pred = net(img) \n",
    "\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss_item = loss.item()\n",
    "        loss_val += loss_item\n",
    "\n",
    "        acc_current = accuracy(pred, label)\n",
    "        acc_val += acc_current #мы же выводим среднее, поэтому суммируем\n",
    "    \n",
    "    pbar.set_description(f'loss: {loss_item:.5f}\\taccuracy: {acc_current:.3f}')\n",
    "\n",
    "  print(loss_val/len(test_loader))\n",
    "  print(acc_val/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 64, 64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = cv2.imread('test_photo.jpg', cv2.IMREAD_COLOR)\n",
    "test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "test_img = test_img.astype(np.float32) #значения пикселей храним в вещественном формате\n",
    "test_img = test_img/255.0 #нормализуем от 0 до 1\n",
    "\n",
    "test_img = cv2.resize(test_img, (64,64), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "test_img = test_img.transpose((2,0,1))\n",
    "test_img = np.expand_dims(test_img, axis=0)\n",
    "\n",
    "\n",
    "test_img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.0723, -7.2177]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ag\\AppData\\Local\\Temp\\ipykernel_9620\\4007300181.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(test_pred).detach().numpy().argmax() #можно закомментировать argmax и посмотреть вероятности\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка работы НС на тестовом изображении\n",
    "#Узнаем, на кого я больше похоже: на кошечку или собачку\n",
    "# 0 - коты, 1 - собаки\n",
    "test_img = torch.from_numpy(test_img)\n",
    "test_pred = net(test_img)\n",
    "print(test_pred)\n",
    "\n",
    "F.softmax(test_pred).detach().numpy().argmax() #можно закомментировать argmax и посмотреть вероятности"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
