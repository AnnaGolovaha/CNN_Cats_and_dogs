{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision as tv\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset2class(torch.utils.data.Dataset): #2 клааса, наследуем \n",
    "    def  __init__(self, path_cats:str, path_dogs:str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_cats = path_cats\n",
    "        self.path_dogs = path_dogs\n",
    "\n",
    "        self.cats_list = sorted(os.listdir(path_cats))\n",
    "        self.dogs_list = sorted(os.listdir(path_dogs))\n",
    "\n",
    "    def __len__(self): #функция для получения длины датасета\n",
    "        return len(self.cats_list) + len(self.dogs_list)\n",
    "\n",
    "    def __getitem__(self,idx): #для получения индексов, чтобы использовать как массив\n",
    "        if idx < len(self.cats_list):\n",
    "            class_id = 0\n",
    "            img_path = os.path.join(self.path_cats, self.cats_list[idx])\n",
    "        else:\n",
    "            class_id = 1\n",
    "            idx-=len(self.cats_list) #чтобы обращаться ко второй папке с 0-го элемента\n",
    "            #обе папки стоят подряд в датасете\n",
    "\n",
    "            img_path = os.path.join(self.path_dogs, self.dogs_list[idx]) #склеиваем два пути в один\n",
    "                                                                    #сначала название папки общей, затем уже конкретно коты или собаки\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = img.astype(np.float32) #значения пикселей храним в вещественном формате\n",
    "        img = img/255.0 #нормализуем от 0 до 1\n",
    "\n",
    "        img = cv2.resize(img, (64,64), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        img = img.transpose((2,0,1))\n",
    "\n",
    "        t_img = torch.from_numpy(img)\n",
    "        t_class_id = torch.tensor(class_id)\n",
    "\n",
    "\n",
    "        return {'img': t_img, 'label' : t_class_id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats_path = '.\\dataset_cats_and_dogs\\\\train\\cats'\n",
    "train_dogs_path = '.\\dataset_cats_and_dogs\\\\train\\dogs'\n",
    "\n",
    "train_ds_cats_dogs = Dataset2class(train_cats_path, train_dogs_path)\n",
    "\n",
    "test_cats_path = '.\\dataset_cats_and_dogs\\\\test\\cats'\n",
    "test_dogs_path = '.\\dataset_cats_and_dogs\\\\test\\dogs'\n",
    "\n",
    "test_ds_cats_dogs = Dataset2class(test_cats_path, test_dogs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(train_ds_cats_dogs[2][0]) #это кортеж, поэтому сначала выбираем номер кортежа, затем элемент в нём\n",
    "# plt.imshow(train_ds_cats_dogs[2]['img']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0720, 0.0782, 0.0784,  ..., 0.0645, 0.0627, 0.0627],\n",
       "         [0.0720, 0.0782, 0.0784,  ..., 0.0645, 0.0627, 0.0627],\n",
       "         [0.0720, 0.0782, 0.0784,  ..., 0.0645, 0.0627, 0.0627],\n",
       "         ...,\n",
       "         [0.1739, 0.2013, 0.1602,  ..., 0.0510, 0.0510, 0.0476],\n",
       "         [0.1804, 0.2138, 0.1499,  ..., 0.0469, 0.0469, 0.0435],\n",
       "         [0.1867, 0.2064, 0.1010,  ..., 0.0431, 0.0431, 0.0398]],\n",
       "\n",
       "        [[0.0759, 0.0821, 0.0824,  ..., 0.0684, 0.0667, 0.0667],\n",
       "         [0.0759, 0.0821, 0.0824,  ..., 0.0684, 0.0667, 0.0667],\n",
       "         [0.0759, 0.0821, 0.0824,  ..., 0.0684, 0.0667, 0.0667],\n",
       "         ...,\n",
       "         [0.1902, 0.2085, 0.1607,  ..., 0.0527, 0.0510, 0.0476],\n",
       "         [0.1978, 0.2210, 0.1504,  ..., 0.0486, 0.0469, 0.0435],\n",
       "         [0.2041, 0.2137, 0.1015,  ..., 0.0449, 0.0431, 0.0398]],\n",
       "\n",
       "        [[0.0955, 0.1017, 0.1020,  ..., 0.0880, 0.0863, 0.0863],\n",
       "         [0.0955, 0.1017, 0.1020,  ..., 0.0880, 0.0863, 0.0863],\n",
       "         [0.0955, 0.1017, 0.1020,  ..., 0.0880, 0.0863, 0.0863],\n",
       "         ...,\n",
       "         [0.2042, 0.2185, 0.1640,  ..., 0.0605, 0.0588, 0.0555],\n",
       "         [0.2112, 0.2299, 0.1515,  ..., 0.0564, 0.0547, 0.0514],\n",
       "         [0.2175, 0.2226, 0.1026,  ..., 0.0527, 0.0510, 0.0476]]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_cats_dogs[2]['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds_cats_dogs, shuffle = True, \n",
    "    batch_size = batch_size, num_workers = 0, drop_last = True\n",
    ") #drop_last = True - выбрасываем последний элемент\n",
    "\n",
    "tesr_loader = torch.utils.data.DataLoader(\n",
    "    test_ds_cats_dogs, shuffle = True, \n",
    "    batch_size = batch_size, num_workers = 1, drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d (2,2)\n",
    "        self.conv0 = nn.Conv2d(3,32,3, stride=1, padding = 0)\n",
    "        self.conv1 = nn.Conv2d(32,32,3, stride=1, padding = 0)\n",
    "        self.conv2 = nn.Conv2d(32,64,3, stride=1, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(64,64,3, stride=1, padding = 0)\n",
    "        self.conv4 = nn.Conv2d(64,64,3, stride=1, padding = 0)\n",
    "\n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1=nn.Linear(64,10)\n",
    "        self.linear2=nn.Linear(10,2) #только 2 класса на выходе\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv0(x)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.act(out)\n",
    "       \n",
    "        out = self.adaptivepool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (act): LeakyReLU(negative_slope=0.2)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (adaptivepool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear1): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (linear2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_loader:\n",
    "    img = sample['img']\n",
    "    label = sample['label']\n",
    "    net(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr =0.0001, betas = (0.9, 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, label):\n",
    "  answer = F.softmax(pred.detach()).numpy().argmax(1) == label.numpy().argmax(1)\n",
    "\n",
    "  # detach() удаляет граф вычислений (как бы историю) с тензора. Отвязывает.\n",
    "# Back propagation не будет идти дальше этого тензора.\n",
    "\n",
    "  return answer.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameers(model):\n",
    "  return sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103168"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameers(net) #число параметров сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]C:\\Users\\ag\\AppData\\Local\\Temp\\ipykernel_11008\\259133754.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  answer = F.softmax(pred.detach()).numpy().argmax(1) == label.numpy().argmax(1)\n",
      "100%|██████████| 34/34 [00:09<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6991277231889612\n",
      "0.49264705882352944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6941955720677095\n",
      "0.5036764705882353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7052505472127129\n",
      "0.5147058823529411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7096734941005707\n",
      "0.49816176470588236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7315180091296926\n",
      "0.5202205882352942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.024965309044894\n",
      "0.49264705882352944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.967220606172786\n",
      "0.5091911764705882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3557054260197807\n",
      "0.5036764705882353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981414770378786\n",
      "0.5110294117647058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:09<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.69448899696855\n",
      "0.47610294117647056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#   loss_val = 0\n",
    "#   acc_val = 0\n",
    "#   # тут будут пачки по 16 сэмплов\n",
    "#   for img, label in (pbar := tqdm(dataloader)): #для каждого изображения считается loss и градиент\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  loss_val = 0 \n",
    "  acc_val = 0\n",
    "  for sample in (pbar := tqdm(train_loader)):\n",
    "    img, label = sample['img'], sample['label']\n",
    "    optimizer.zero_grad()  #обновляем градиенты\n",
    "\n",
    "\n",
    "    label = (torch.Tensor(F.one_hot(label, 2))).float() \n",
    "\n",
    "\n",
    "    pred = net(img) #здесь 10 значений (для каждого класса) в 16 тензорах(размер батча)\n",
    "\n",
    "    loss = loss_fn(pred, label)\n",
    "\n",
    "    loss.backward()\n",
    "    loss_item = loss.item()\n",
    "    loss_val += loss_item\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    acc_current = accuracy(pred, label)\n",
    "    acc_val += acc_current #мы же выводим среднее, поэтому суммируем\n",
    "\n",
    "\n",
    "  \n",
    "  pbar.set_description(f'loss: {loss_item:.5f}\\taccuracy: {acc_current:.3f}')\n",
    "\n",
    "  print(loss_val/len(train_loader))\n",
    "  print(acc_val/len(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
